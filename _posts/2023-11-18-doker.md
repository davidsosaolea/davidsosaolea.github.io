---
layout: single
title: '🚢 Docker para Científicos de Datos y mas 🚢'
excerpt: "Docker es una plataforma que permite desarrollar, implementar y ejecutar aplicaciones dentro de contenedores. Un contenedor es una unidad estandarizada de software que empaqueta el código de una aplicación y todas sus dependencias (bibliotecas, configuraciones, etc.) para que pueda ejecutarse de manera rápida y confiable en cualquier entorno."
date: "2024-10-08"
classes: wide
header:
  teaser: assets/images/htb-writeup-Doker/doker.jpeg
  teaser_home_page: true
  icon: /assets/images/hackthebox.webp
categories:
  - Analysis of data
tags:  
  - Linux
  - Doker

---

# 🚢 **Docker para Científicos de Datos: El Barco que Flota tus Modelos** 🚢

Imagina que estás en un barco llamado **Data Science Express**. Estás navegando tranquilamente con tus modelos de Machine Learning, tus gráficos de visualización y todo tu entorno Python perfectamente configurado. De repente, necesitas mover ese barco a otro océano (o servidor). ¿Qué pasa? ¡Se inunda! No tienes las mismas dependencias, ni la misma configuración, y tu código naufraga en la confusión.

Aquí es donde Docker, nuestro **capitán con gorra y pipa**, llega al rescate. Docker es como una cápsula del tiempo para tu entorno. Te permite empacar tu código, dependencias, librerías y hasta el sistema operativo en un contenedor hermético que puede navegar por cualquier mar digital. Así que, ya sea que estés ejecutando tu modelo de Machine Learning en tu laptop o en un servidor en la otra punta del mundo, ¡todo funcionará igual de perfecto!

### 🐋 ¿Pero qué es Docker?

Docker es una herramienta que **"conteneriza"** tu entorno, lo que significa que puedes crear imágenes que contengan todo lo que necesita tu aplicación para ejecutarse. Así evitas el clásico: “**¡En mi máquina funciona!**” y permite a cualquier equipo reproducir tus experimentos sin importar su configuración.

### 🎯 ¿Por qué Docker en Ciencia de Datos?

Docker es **el mejor amigo** de los científicos de datos por varias razones:

1. **Reproducibilidad**: ¡Nada más frustrante que un código que funciona en tu computadora y se rompe en el servidor de producción! Con Docker, empaquetas todo, desde dependencias hasta configuraciones, para que funcione en cualquier lado.
2. **Facilidad de compartir**: ¿Trabajas en equipo? ¿Quieres enviar tu proyecto a un amigo para que lo revise? ¡No hay problema! Solo le das tu imagen de Docker y listo.
3. **Entornos limpios**: ¿Estás harto de tener 10 versiones diferentes de Python y R peleando por la atención de tu sistema operativo? Docker te permite mantener entornos separados sin que se interfieran entre sí.
4. **Despliegue sencillo**: Ya sea que estés desarrollando localmente o desplegando tu modelo en la nube, Docker te permite hacerlo de manera rápida y eficiente.

### 🛠️ ¡Manos a la obra! Creando un contenedor Docker para un proyecto de Ciencia de Datos

Vamos a armar un ejemplo de cómo usar Docker para un proyecto de Machine Learning que predice si un pasajero del Titanic sobrevivió o no. 

#### 1. **Primer paso: Tu código de Python**

Escribe tu código en un archivo `titanic_model.py`:

```python
# titanic_model.py

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Cargar datos del Titanic
data = pd.read_csv("titanic.csv")

# Preprocesamiento básico
data = data[['Pclass', 'Sex', 'Age', 'Survived']].dropna()
data['Sex'] = data['Sex'].apply(lambda x: 1 if x == 'male' else 0)

# Separar características y objetivo
X = data[['Pclass', 'Sex', 'Age']]
y = data['Survived']

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar el modelo
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Hacer predicciones y evaluar el modelo
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy del modelo: {accuracy}")
```

#### 2. **Dockerfile: Construyendo el barco**

Ahora, necesitamos crear un archivo llamado `Dockerfile` que le diga a Docker cómo construir nuestro contenedor:

```dockerfile
# Usamos la imagen oficial de Python
FROM python:3.9-slim

# Establecemos el directorio de trabajo
WORKDIR /app

# Copiamos los archivos de nuestro proyecto al contenedor
COPY titanic_model.py ./
COPY titanic.csv ./

# Instalamos las dependencias necesarias
RUN pip install pandas scikit-learn

# Ejecutamos el script de predicción
CMD ["python", "titanic_model.py"]
```

#### 3. **Construyendo el contenedor**

¡Vamos a construir nuestro contenedor!

En tu terminal, navega hasta el directorio donde tienes el archivo `Dockerfile` y el código de Python. Luego, ejecuta:

```bash
docker build -t titanic-model .
```

Este comando construirá la imagen de Docker y la etiquetará como `titanic-model`.

#### 4. **Correr el contenedor**

Finalmente, ejecutamos nuestro contenedor con el siguiente comando:

```bash
docker run titanic-model
```

¡Y listo! Tu modelo de predicción del Titanic se ejecutará en su propio entorno seguro, encapsulado en un contenedor Docker, sin importar en qué sistema operativo estés.

### 📦 Docker y el Futuro de la Ciencia de Datos

Docker es una herramienta imprescindible para científicos de datos que trabajan en equipos grandes, en múltiples proyectos o que necesitan compartir su trabajo de manera eficiente. Además, es increíblemente útil para automatizar tareas en producción, donde puedes desplegar tus modelos y sistemas sin preocuparte por las dependencias.

**Imagina** un futuro donde ya no tendrás que decir “en mi máquina funciona” y en vez de eso, serás el capitán de un barco de datos que navega tranquilo, sin tormentas de dependencias ni mares de errores.

¡Sube a bordo del barco de Docker y navega hacia el éxito en tus proyectos de ciencia de datos! 🌊🚢


