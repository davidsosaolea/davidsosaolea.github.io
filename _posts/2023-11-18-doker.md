---
layout: single
title: 'ğŸš¢ Docker para CientÃ­ficos de Datos y mas ğŸš¢'
excerpt: "Docker es una plataforma que permite desarrollar, implementar y ejecutar aplicaciones dentro de contenedores. Un contenedor es una unidad estandarizada de software que empaqueta el cÃ³digo de una aplicaciÃ³n y todas sus dependencias (bibliotecas, configuraciones, etc.) para que pueda ejecutarse de manera rÃ¡pida y confiable en cualquier entorno."
date: "2024-10-08"
classes: wide
header:
  teaser: assets/images/htb-writeup-Doker/doker.jpeg
  teaser_home_page: true
  icon: /assets/images/hackthebox.webp
categories:
  - Analysis of data
tags:  
  - Linux
  - Doker

---

# ğŸš¢ **Docker para CientÃ­ficos de Datos: El Barco que Flota tus Modelos** ğŸš¢

Imagina que estÃ¡s en un barco llamado **Data Science Express**. EstÃ¡s navegando tranquilamente con tus modelos de Machine Learning, tus grÃ¡ficos de visualizaciÃ³n y todo tu entorno Python perfectamente configurado. De repente, necesitas mover ese barco a otro ocÃ©ano (o servidor). Â¿QuÃ© pasa? Â¡Se inunda! No tienes las mismas dependencias, ni la misma configuraciÃ³n, y tu cÃ³digo naufraga en la confusiÃ³n.

AquÃ­ es donde Docker, nuestro **capitÃ¡n con gorra y pipa**, llega al rescate. Docker es como una cÃ¡psula del tiempo para tu entorno. Te permite empacar tu cÃ³digo, dependencias, librerÃ­as y hasta el sistema operativo en un contenedor hermÃ©tico que puede navegar por cualquier mar digital. AsÃ­ que, ya sea que estÃ©s ejecutando tu modelo de Machine Learning en tu laptop o en un servidor en la otra punta del mundo, Â¡todo funcionarÃ¡ igual de perfecto!

### ğŸ‹ Â¿Pero quÃ© es Docker?

Docker es una herramienta que **"conteneriza"** tu entorno, lo que significa que puedes crear imÃ¡genes que contengan todo lo que necesita tu aplicaciÃ³n para ejecutarse. AsÃ­ evitas el clÃ¡sico: â€œ**Â¡En mi mÃ¡quina funciona!**â€ y permite a cualquier equipo reproducir tus experimentos sin importar su configuraciÃ³n.

### ğŸ¯ Â¿Por quÃ© Docker en Ciencia de Datos?

Docker es **el mejor amigo** de los cientÃ­ficos de datos por varias razones:

1. **Reproducibilidad**: Â¡Nada mÃ¡s frustrante que un cÃ³digo que funciona en tu computadora y se rompe en el servidor de producciÃ³n! Con Docker, empaquetas todo, desde dependencias hasta configuraciones, para que funcione en cualquier lado.
2. **Facilidad de compartir**: Â¿Trabajas en equipo? Â¿Quieres enviar tu proyecto a un amigo para que lo revise? Â¡No hay problema! Solo le das tu imagen de Docker y listo.
3. **Entornos limpios**: Â¿EstÃ¡s harto de tener 10 versiones diferentes de Python y R peleando por la atenciÃ³n de tu sistema operativo? Docker te permite mantener entornos separados sin que se interfieran entre sÃ­.
4. **Despliegue sencillo**: Ya sea que estÃ©s desarrollando localmente o desplegando tu modelo en la nube, Docker te permite hacerlo de manera rÃ¡pida y eficiente.

### ğŸ› ï¸ Â¡Manos a la obra! Creando un contenedor Docker para un proyecto de Ciencia de Datos

Vamos a armar un ejemplo de cÃ³mo usar Docker para un proyecto de Machine Learning que predice si un pasajero del Titanic sobreviviÃ³ o no. 

#### 1. **Primer paso: Tu cÃ³digo de Python**

Escribe tu cÃ³digo en un archivo `titanic_model.py`:

```python
# titanic_model.py

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Cargar datos del Titanic
data = pd.read_csv("titanic.csv")

# Preprocesamiento bÃ¡sico
data = data[['Pclass', 'Sex', 'Age', 'Survived']].dropna()
data['Sex'] = data['Sex'].apply(lambda x: 1 if x == 'male' else 0)

# Separar caracterÃ­sticas y objetivo
X = data[['Pclass', 'Sex', 'Age']]
y = data['Survived']

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar el modelo
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Hacer predicciones y evaluar el modelo
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy del modelo: {accuracy}")
```

#### 2. **Dockerfile: Construyendo el barco**

Ahora, necesitamos crear un archivo llamado `Dockerfile` que le diga a Docker cÃ³mo construir nuestro contenedor:

```dockerfile
# Usamos la imagen oficial de Python
FROM python:3.9-slim

# Establecemos el directorio de trabajo
WORKDIR /app

# Copiamos los archivos de nuestro proyecto al contenedor
COPY titanic_model.py ./
COPY titanic.csv ./

# Instalamos las dependencias necesarias
RUN pip install pandas scikit-learn

# Ejecutamos el script de predicciÃ³n
CMD ["python", "titanic_model.py"]
```

#### 3. **Construyendo el contenedor**

Â¡Vamos a construir nuestro contenedor!

En tu terminal, navega hasta el directorio donde tienes el archivo `Dockerfile` y el cÃ³digo de Python. Luego, ejecuta:

```bash
docker build -t titanic-model .
```

Este comando construirÃ¡ la imagen de Docker y la etiquetarÃ¡ como `titanic-model`.

#### 4. **Correr el contenedor**

Finalmente, ejecutamos nuestro contenedor con el siguiente comando:

```bash
docker run titanic-model
```

Â¡Y listo! Tu modelo de predicciÃ³n del Titanic se ejecutarÃ¡ en su propio entorno seguro, encapsulado en un contenedor Docker, sin importar en quÃ© sistema operativo estÃ©s.

### ğŸ“¦ Docker y el Futuro de la Ciencia de Datos

Docker es una herramienta imprescindible para cientÃ­ficos de datos que trabajan en equipos grandes, en mÃºltiples proyectos o que necesitan compartir su trabajo de manera eficiente. AdemÃ¡s, es increÃ­blemente Ãºtil para automatizar tareas en producciÃ³n, donde puedes desplegar tus modelos y sistemas sin preocuparte por las dependencias.

**Imagina** un futuro donde ya no tendrÃ¡s que decir â€œen mi mÃ¡quina funcionaâ€ y en vez de eso, serÃ¡s el capitÃ¡n de un barco de datos que navega tranquilo, sin tormentas de dependencias ni mares de errores.

Â¡Sube a bordo del barco de Docker y navega hacia el Ã©xito en tus proyectos de ciencia de datos! ğŸŒŠğŸš¢


